
[{"content":" This website is a personal knowledge base focused on DevOps, cloud infrastructure, and automation. It contains practical notes, tutorials, and quick reference guides covering cloud services, Kubernetes, and containerized workloads. Much of the content is based on hands-on experience, real-world problem solving, and homelab experimentation. The goal of this site is to document lessons learned, simplify complex concepts, and provide a reliable reference for engineers working with modern cloud and DevOps technologies. Happy Hosting! ðŸš€\nGo to Blogs ","date":"19 December 2025","externalUrl":null,"permalink":"/","section":"","summary":"","title":"","type":"page"},{"content":"","date":"19 December 2025","externalUrl":null,"permalink":"/blogs/","section":"","summary":"","title":"","type":"blogs"},{"content":"Alright, let\u0026rsquo;s break down Route 53 in a way that makes sense for us DevOps folks. Think of it as your DNS traffic cop for AWS.\nWhat\u0026rsquo;s Route 53 All About? #\rRoute 53 is AWS\u0026rsquo;s DNS service (and yes, \u0026ldquo;53\u0026rdquo; is the DNS port number). It handles domain registration, routes your internet traffic to the right place, and keeps tabs on whether your stuff is actually working. Pretty cool, right?\nKey facts:\nOne of AWS\u0026rsquo;s most reliable services (seriously, they claim 100% uptime) Works with any AWS resource (EC2, ALBs, S3, CloudFront, etc.) Can handle on-premises infrastructure too Global distribution means your DNS queries are super fast Hosted Zones: Your DNS Home Base #\rA hosted zone is basically a container that holds all your DNS records for a domain.\nPublic Hosted Zone Handles traffic coming from the internet. Use this when you need publicly accessible resources.\nPrivate Hosted Zone Lives inside your VPC and handles DNS for your internal resources. Perfect for stuff you don\u0026rsquo;t want exposed to the internet.\nWhen you create a hosted zone, Route 53 automatically creates NS (name server) and SOA (start of authority) records. You get four unique name servers for delegation.\nDNS Record Types You\u0026rsquo;ll Actually Use #\rRecord Type What It Does Example A Maps domain to IPv4 address example.com â†’ 192.0.2.1 AAAA Maps domain to IPv6 address example.com â†’ 2001:0db8::1 CNAME Points domain to another domain blog.example.com â†’ example.com MX Routes email to mail servers Points to your mail server TXT Holds arbitrary text data SPF, DKIM, domain verification NS Specifies authoritative name servers Created automatically SOA Start of Authority record Created automatically Alias Records (Route 53\u0026rsquo;s Secret Weapon) #\rAlias records are like CNAME but better. They\u0026rsquo;re AWS-specific and let you point to AWS resources without weird DNS issues.\nWhy they\u0026rsquo;re awesome:\nCan be used at the root domain (example.com, not just subdomains) No extra DNS lookup charges Work with ALBs, CloudFront, S3, and other AWS resources Return an A record to clients (server-side resolution) example.com: Type: A AliasTarget: HostedZoneId: Z35SXDOTRQ7X7K # ALB Zone ID DNSName: my-alb-123456.us-east-1.elb.amazonaws.com EvaluateTargetHealth: true The 8 Routing Policies (Pick Your Strategy) #\r1. Simple Routing #\rDefault option. Send traffic to a single resource. No frills.\nUse when: You\u0026rsquo;ve got one resource and don\u0026rsquo;t need failover Example: Single EC2 instance 2. Weighted Routing #\rSplit traffic based on percentages you set.\nUse when: A/B testing, gradual migrations, or blue-green deployments Example: 70% to us-east-1, 30% to eu-west-1 Resource1: Weight: 70 Resource2: Weight: 30 3. Latency Routing #\rRoute users to the resource with the lowest latency for them.\nUse when: Multi-region setup where response time matters Example: EU users hit eu-west-1, US users hit us-east-1 4. Failover Routing (Active-Passive) #\rPrimary resource handles traffic. Secondary takes over if primary goes down.\nUse when: Simple HA setup (not load balancing) Example: Primary database in us-east-1, failover in us-west-2 5. Geolocation Routing #\rRoute based on where your users actually are (continent, country, state).\nUse when: Content delivery, compliance, or geo-specific data Example: EU users get EU data center, compliance restrictions, etc. 6. Geoproximity Routing #\rCombines user location and resource location with optional bias.\nUse when: You want geographic preference but with flexibility Bias adjustments expand or shrink your service areas 7. Multivalue Answer Routing #\rReturn up to 8 random healthy records at once.\nUse when: Simple load balancing without complexity Not a replacement for actual load balancers Each record gets its own health check 8. IP Based Routing #\rRoute traffic based on the client\u0026rsquo;s IP address (CIDR blocks).\nUse when: You know the IP ranges and want custom routing Use case: Different treatment for your corporate office vs public traffic Health Checks: Making Sure Everything\u0026rsquo;s Actually Healthy #\rHealth checks monitor your resources and automatically redirect traffic if something\u0026rsquo;s broken.\nThree Types of Health Checks #\rEndpoint Health Checks Monitor actual endpoints (HTTP/HTTPS, TCP, etc.)\nConfigure intervals: Standard (30 seconds) or Fast (10 seconds) Set failure threshold before marking unhealthy Can search for specific text in responses HealthCheck: Type: HTTP IPAddress: 192.0.2.1 Port: 80 Path: /health RequestInterval: 30 FailureThreshold: 3 SearchString: \u0026#34;OK\u0026#34; Calculated Health Checks Aggregate the status of other health checks\nCombine multiple health check results Great for: Complex availability logic CloudWatch Health Checks Use CloudWatch alarms as health check sources\nMonitor any CloudWatch metric Trigger failover based on custom metrics Key Health Check Features #\rGlobal monitoring from multiple AWS edge locations (prevents false positives) Automatic CloudWatch metrics integration Custom headers and request configuration Text string matching in responses SNS notifications and alarms on status change Route 53 Resolver: Your AWS DNS Middleman #\rHandles DNS resolution both ways between AWS and on-premises.\nInbound Endpoints On-premises systems query Route 53 for AWS resources\nOutbound Endpoints AWS resources query Route 53 for on-premises DNS\nPerfect for hybrid environments where you need seamless DNS.\nCommon Patterns You\u0026rsquo;ll Actually Use #\rBlue-Green Deployment #\rUse weighted routing, slowly shift traffic from blue to green:\nBlueRecords: Weight: 100 GreenRecords: Weight: 0 # Gradually increase Green weight, decrease Blue weight Regional Failover #\rLatency routing handles primary, failover takes secondary:\nPrimary: Region: us-east-1 HealthCheck: primary-hc SetIdentifier: primary Secondary: Region: us-west-2 HealthCheck: secondary-hc SetIdentifier: secondary Traffic Distribution #\rWeighted routing for distributing load across regions:\nRegionA: Weight: 40 RegionB: Weight: 40 RegionC: Weight: 20 DNS Resolution Flow (Quick Mental Model) #\rUser asks browser: \u0026ldquo;What\u0026rsquo;s the IP for example.com?\u0026rdquo; Browser queries recursive resolver (ISP\u0026rsquo;s DNS) Resolver queries .com authoritative servers Gets Route 53 name server addresses Queries Route 53 with your routing policy Route 53 applies logic (latency, weight, geo, etc.) Returns the right IP address Browser connects to that IP Things That\u0026rsquo;ll Save You Time #\rEnable Health Check CloudWatch Alarms Get SNS notifications before your users notice problems\nUse Alias Records Save money (no extra charges) and avoid CNAME issues at root\nSet Up Traffic Flow (If you need it) Visual editor for complex routing policies. Easier than managing individual records.\nTag Your Resources Makes cleanup and organization so much easier\nMonitor Resolver Query Logs Debug DNS issues without tearing your hair out\nImportant Gotchas #\rCNAME records can\u0026rsquo;t be used at domain root (use Alias instead) Health checks run from multiple locations (prevents false positives but means you need proper metrics) Private hosted zones need enableDnsHostnames and enableDnsSupport on your VPC Failover routing only works in public hosted zones Default limit is 200 health checks (request increase if needed) Health check intervals are fixed (30 or 10 seconds, nothing in between) Quick Pricing Note #\rRoute 53 is cheap. You pay per hosted zone and per query (after free tier). Health checks add a small cost. Domain registration is separate and varies by TLD.\nHope you found this helpful. Happy hosting!\n","date":"19 December 2025","externalUrl":null,"permalink":"/blogs/blog9-aws-route53/","section":"","summary":"We deep dive into AWS Route 53: hosted zones, DNS records/Aliases, routing policies, health checks, and more.","title":"AWS Route 53 Quick Reference Guide","type":"blogs"},{"content":"Hello world! If you\u0026rsquo;ve been working with AWS, you\u0026rsquo;ve probably bumped into CIDR notation at some point. Whether you\u0026rsquo;re setting up VPCs, security groups, or route tables, CIDR blocks are everywhere. I\u0026rsquo;ll be honest, when I first started my DevOps journey, CIDR notation felt like absolute magic. But once it clicked for me, it became one of my go-to tools for designing robust cloud infrastructure.\nLet me walk you through what CIDR actually is, how to calculate it, and show you some real world situations where I\u0026rsquo;ve used it in my AWS environments.\nWhat Even Is CIDR? #\rCIDR stands for Classless Inter-Domain Routing. I know, fancy name. But here\u0026rsquo;s the thing: it\u0026rsquo;s just a shorthand way to describe a range of IP addresses. Instead of saying \u0026ldquo;all IP addresses from 10.0.0.0 to 10.0.0.255,\u0026rdquo; you can just write 10.0.0.0/24 and boom, you\u0026rsquo;ve described the same thing in a much cleaner way.\nThe Basics: How CIDR Works #\rAn IPv4 address is made up of 32 bits total. These 32 bits are split into two parts: the network part and the host part. The CIDR notation (the /xx number) tells you exactly where that split happens.\nFor example, in 10.0.0.0/24, the /24 means:\nFirst 24 bits = the network part (this stays the same for all addresses in the range) Last 8 bits = the host part (this can vary) The network part identifies which subnet you\u0026rsquo;re in. The host part identifies individual devices within that subnet.\nLet me break down 10.0.0.0/24 in binary to make it clearer:\n10.0.0.0 in binary = 00001010.00000000.00000000.00000000 /24 means first 24 bits are fixed Network part (first 24 bits): 00001010.00000000.00000000 (this is always the same) Host part (last 8 bits): .00000000 to .11111111 (these can vary) So the range is: 10.0.0.0 to 10.0.0.255 Simple, right? The /24 just tells you where the dividing line is between the network and host portions.\nHow to Calculate CIDR Ranges #\rHere\u0026rsquo;s the formula I use constantly. It\u0026rsquo;s super simple:\nHost bits = 32 minus CIDR number\nNumber of addresses = 2^(host bits)\nLet me walk through a few examples so you see how it works.\nFor /24:\nHost bits = 32 - 24 = 8 bits Number of addresses = 2^8 = 256 addresses Usable IPs = 256 minus 2 (network and broadcast) = 254 usable IPs For /16:\nHost bits = 32 - 16 = 16 bits Number of addresses = 2^16 = 65,536 addresses Usable IPs = 65,536 minus 2 = 65,534 usable IPs For /30:\nHost bits = 32 - 30 = 2 bits Number of addresses = 2^2 = 4 addresses Usable IPs = 4 minus 2 = 2 usable IPs For /32:\nHost bits = 32 - 32 = 0 bits Number of addresses = 2^0 = 1 address (a single host) Usable IPs = 1 (it\u0026rsquo;s just one IP, no network or broadcast address) Common CIDR Notations at a Glance #\rHere\u0026rsquo;s a quick reference of the ones you\u0026rsquo;ll see most often:\n/32 = 1 IP address (single host, super specific) /30 = 4 IP addresses (2 usable, great for point to point links) /24 = 256 IP addresses (254 usable, perfect for small to medium subnets) /16 = 65,536 IP addresses (65,534 usable, great for large networks) /8 = 16,777,216 IP addresses (huge networks, rarely used in AWS) Why Does CIDR Matter? #\rOnce you understand CIDR, you can design networks that are exactly the right size. Too small, and you run out of IPs. Too big, and you\u0026rsquo;re wasting address space. CIDR lets you be precise about it.\nIn AWS, everything uses CIDR. Your VPC needs a CIDR block. Each subnet needs a CIDR block. Security groups use CIDR to define what traffic is allowed. Route tables use CIDR to decide where packets should go. So getting comfortable with this notation early on saves you tons of confusion later.\nHow I Use CIDR in AWS #\rLet me give you a real world example from one of my home lab projects. I was setting up a VPC in AWS to run a few microservices in Kubernetes, and I needed to plan my network carefully.\nCreating a VPC and Subnets #\rWhen I created my VPC, I decided to use 10.0.0.0/16 as my overall network block. This gave me plenty of room to work with. Then I carved that up into smaller subnets:\nVPC: CIDR: 10.0.0.0/16 Total IPs: 65,536 (254 usable per subnet once divided) Subnets: PublicSubnet1: CIDR: 10.0.1.0/24 IPs: 256 (254 usable) AZ: us-east-1a PublicSubnet2: CIDR: 10.0.2.0/24 IPs: 256 (254 usable) AZ: us-east-1b PrivateSubnet1: CIDR: 10.0.10.0/24 IPs: 256 (254 usable) AZ: us-east-1a PrivateSubnet2: CIDR: 10.0.11.0/24 IPs: 256 (254 usable) AZ: us-east-1b Notice how I spaced things out? I used /24 for each subnet, which gives me 256 IPs per subnet. For my use case, that\u0026rsquo;s more than enough. I could spin up an EKS cluster, a few RDS instances, and still have room to grow.\nThe beauty of this approach is that I\u0026rsquo;m not wasting the address space. By choosing appropriate CIDR blocks, I can scale my infrastructure without having to redesign my entire VPC later.\nSetting Up Security Groups #\rThis is where CIDR really shines for me. Security groups in AWS use CIDR notation to control who can talk to what. Let me walk you through a scenario.\nI had an EKS cluster that needed to talk to an RDS database. Instead of just opening port 5432 to the entire internet (please don\u0026rsquo;t do that!), I used the CIDR block of my private subnets where the EKS nodes lived.\nRDSSecurityGroup: InboundRules: - Protocol: TCP Port: 5432 Source: 10.0.10.0/24 Description: \u0026#34;Allow from EKS nodes in AZ-1a\u0026#34; - Protocol: TCP Port: 5432 Source: 10.0.11.0/24 Description: \u0026#34;Allow from EKS nodes in AZ-1b\u0026#34; This way, only my Kubernetes nodes can reach the database. It\u0026rsquo;s secure, precise, and scales with my infrastructure.\nManaging Kubernetes Pod Networks with CIDR #\rWhen I deployed EKS on AWS, I needed to think carefully about CIDR blocks for the pods themselves. My VPC uses 10.0.0.0/16, but the Kubernetes pods need their own IP space. I configured my cluster with a pod CIDR of 100.64.0.0/16, which is completely separate from my VPC network.\nEKSCluster: VPC_CIDR: 10.0.0.0/16 Pod_CIDR: 100.64.0.0/16 Nodes: - Node1_VPC_IP: 10.0.1.50 (from subnet 10.0.1.0/24) - Node2_VPC_IP: 10.0.2.75 (from subnet 10.0.2.0/24) Pods: - Pod1_IP: 100.64.0.10 (from pod CIDR) - Pod2_IP: 100.64.0.11 (from pod CIDR) - Pod3_IP: 100.64.1.5 (from pod CIDR) This separation is clever. My worker nodes live in the VPC subnet (10.0.x.x), but the pods they run get IPs from a completely different range (100.64.x.x). This prevents IP conflicts and gives me tons of flexibility. I can have hundreds of pods running on just a few nodes without worrying about running out of IPs in my subnets.\nManaging Database Replication with CIDR Rules #\rHere\u0026rsquo;s a practical example I ran into recently. I have an RDS instance for production in one AWS region and a read replica in another region. I needed to allow replication traffic between them using specific CIDR blocks.\nMy production database is in us-east-1 with a security group that has a CIDR block of 10.0.0.0/16. My replica is in us-west-2 with a different VPC using 10.1.0.0/16. I set up an inter-region VPC peering connection and configured the security groups like this:\nProductionRDSSecurityGroup: InboundRules: - Protocol: TCP Port: 3306 Source: 10.1.0.0/16 Description: \u0026#34;Allow MySQL replication from us-west-2 replica VPC\u0026#34; ReplicaRDSSecurityGroup: InboundRules: - Protocol: TCP Port: 3306 Source: 10.0.0.0/16 Description: \u0026#34;Allow read queries from production VPC\u0026#34; By using the VPC CIDR blocks instead of individual IP addresses, the security groups automatically work for any database instance I launch in those VPCs. If I add new instances later, the rules still apply. It\u0026rsquo;s much more scalable than hardcoding individual IPs.\nCIDR Quick Reference Guide #\rHere\u0026rsquo;s a handy cheatsheet I keep bookmarked for quick lookups. You\u0026rsquo;ll end up memorizing these pretty quickly, but when you need a fast reference, this is it.\nCIDR Notations and Host Count #\rCIDR Notation Total Addresses Usable IPs Common Use Case /32 1 1 Single host, specific IP /31 2 2 Point to point links (RFC 3021) /30 4 2 VPN tunnels, router links /29 8 6 Small lab environment /28 16 14 Tiny subnet /27 32 30 Small subnet /26 64 62 Medium subnet /25 128 126 Medium subnet /24 256 254 Standard subnet (AWS default for many services) /23 512 510 Larger subnet /22 1,024 1,022 Large subnet /21 2,048 2,046 Very large subnet /20 4,096 4,094 Very large subnet /16 65,536 65,534 Typical VPC size /15 131,072 131,070 Large VPC /8 16,777,216 16,777,214 Entire RFC 1918 block Quick Calculation Cheat #\rIf you need to calculate on the fly, remember this formula:\nNumber of Addresses = 2^(32 - CIDR)\nUsable IPs = 2^(32 - CIDR) - 2 (subtract 2 for network and broadcast addresses)\nExamples:\n/24: 2^(32-24) = 2^8 = 256 addresses (254 usable) /16: 2^(32-16) = 2^16 = 65,536 addresses (65,534 usable) /22: 2^(32-22) = 2^10 = 1,024 addresses (1,022 usable) RFC 1918 Private IP Ranges #\rAlways use these for your internal networks. They\u0026rsquo;re reserved for private use and won\u0026rsquo;t conflict with the public internet.\n10.0.0.0/8 - Largest private range, 16.7 million addresses 172.16.0.0/12 - Medium private range, 1 million addresses 192.168.0.0/16 - Smallest private range, 65,536 addresses AWS Default Recommendations #\rWhen I\u0026rsquo;m setting up a new VPC, I typically follow this pattern:\nVPC CIDR: 10.0.0.0/16 (gives me 65,536 IPs to work with) Public Subnet 1: 10.0.1.0/24 (254 usable IPs) Public Subnet 2: 10.0.2.0/24 (254 usable IPs) Private Subnet 1: 10.0.10.0/24 (254 usable IPs) Private Subnet 2: 10.0.11.0/24 (254 usable IPs) EKS Pod CIDR: 100.64.0.0/16 (separate from VPC, plenty of room for pods) This layout leaves me with plenty of unused CIDR blocks for future growth without having to redesign my network.\nCommon CIDR Mistakes to Avoid #\rDo not overlap CIDR blocks within the same VPC Do not use overlapping ranges when peering multiple VPCs Always account for the two reserved addresses (network and broadcast) Do not allocate all your address space at once, leave room for growth Do not peer VPCs with conflicting CIDR ranges Hope you found this helpful. Happy hosting!\n","date":"19 December 2025","externalUrl":null,"permalink":"/blogs/blog8-cidrs/","section":"","summary":"","title":"CIDR Basics and Practical Use Cases in AWS","type":"blogs"},{"content":"Getting your code to production shouldn\u0026rsquo;t feel like launching a rocket. Different strategies fit different situations, and knowing which one to reach for can mean the difference between a smooth Friday afternoon and a chaotic one. Let\u0026rsquo;s break down the most common release strategies and help you pick the right fit for your team.\nRelease vs Deployment: What\u0026rsquo;s the Difference? #\rBefore we dive into the strategies, let\u0026rsquo;s clear up what we mean by these terms because they\u0026rsquo;re often used interchangeably but they\u0026rsquo;re not quite the same thing.\nDeployment is the technical process of moving code to a server. It\u0026rsquo;s pushing bits from point A to point B. Think of it as the mechanics of getting your application running on infrastructure.\nRelease is the business decision to make something available to users. You might deploy multiple times before you actually release to customers, or you might release something that\u0026rsquo;s been deployed for days. Release is about user visibility and impact.\nIn practice, DevOps has blurred these lines with continuous deployment strategies that automate much of the deployment process, but the distinction still matters when you\u0026rsquo;re planning your rollout.\nRecreate Deployment: The Clean Slate Approach #\rThis is the simplest strategy and sometimes the best one. You stop all the old instances, then start all the new ones. It\u0026rsquo;s straightforward but comes with downtime.\nRecreate deployment works like this:\nOld version is running. You take everything offline. New version starts. Users see a brief interruption.\nWhen should you use it? When your application can tolerate a few minutes offline, when you have breaking database schema changes that need to run before the app starts, or for non production environments where you\u0026rsquo;re testing major updates.\nThe trade off is obvious: you get simplicity and reliability, but you lose availability. Plan your recreation deployment carefully, ideally during low traffic windows, and make sure your team knows when it\u0026rsquo;s happening.\nRolling Deployments: The Gradual Update #\rRolling deployments are the default in Kubernetes for good reason. Instead of taking everything down, you update a few instances at a time while the rest handle traffic. It\u0026rsquo;s like replacing the engine of a car while it\u0026rsquo;s still driving.\nHere\u0026rsquo;s how it works:\nYou have 10 instances running version 1.0. You spin up 2 new instances running version 2.0. Once they\u0026rsquo;re healthy, you start routing traffic to them. You shut down 2 old instances. You repeat this process until all instances are on version 2.0.\nThroughout this process, users keep getting served by healthy instances. If something goes wrong with the new version, you\u0026rsquo;ve only affected a small slice of your traffic. Kubernetes handles most of this automatically through its rolling update mechanism.\nRolling deployments are your go to for most applications. They give you continuous availability, allow for quick rollbacks if needed, and work well with health checks that validate each batch before moving forward.\nThe catch? It takes longer than a blue green deployment, and you need to handle database migrations carefully since you might have two versions talking to the same database.\nCanary Deployment: Real World Testing #\rCanary deployments take the rolling approach and add validation. You deploy the new version to a small percentage of users (say 5%), monitor it intensely, and if all looks good, gradually increase the traffic.\nThe process:\nNew version goes live to 5% of users. Your monitoring systems watch for errors, latency spikes, and any other anomalies. If everything is stable after an hour, bump it to 25%. After another hour, 50%. Keep going until 100%.\nThis strategy is perfect for major features or risky updates. You get early warning signals from real users with minimal blast radius. You also get valuable feedback before full rollout. If you spot an issue at 5%, you\u0026rsquo;ve only impacted a tiny slice of users instead of everyone.\nThe downside is complexity. You need sophisticated monitoring, traffic routing, and the ability to roll back quickly. You also need to think about how long to stay at each percentage, what metrics trigger a rollout pause, and how to handle persistent state across versions.\nBlue Green Deployments: The Safe Swap #\rBlue green deployment maintains two complete, identical production environments. At any moment, one is active (blue) handling traffic, and one is inactive (green). When you\u0026rsquo;re ready to release, you deploy to green, run all your tests there, then flip traffic to green. If something breaks, you flip back to blue.\nThe flow:\nBlue is live with version 1.0. You deploy version 2.0 to green. Green gets smoke tested, health checked, and validated. Load balancer switches all traffic from blue to green. Green is now live.\nIf users report issues, you flip the switch back to blue in seconds. Blue green deployments give you the fastest rollback possible and let you test thoroughly before going live. There\u0026rsquo;s no version mixing during the transition since you completely switch environments at once.\nThe trade off is resource intensive. You need double the infrastructure to run both environments. It also doesn\u0026rsquo;t work well for applications with stateful components unless you handle session synchronization carefully. And if your deployment process takes an hour, you\u0026rsquo;re sitting with green fully deployed before switchover, which can feel risky if the code is ready but not actually handling production traffic.\nShadow Deployments: The Silent Observer #\rShadow deployments run the new version alongside production but send it a copy of real traffic without affecting actual users. You\u0026rsquo;re essentially testing against production data without production consequences.\nHow it works:\nProduction traffic comes in. Load balancer routes real traffic to the current version. Simultaneously, a copy of that traffic goes to the new version. Both versions process the request. Users only see the response from the current version.\nYou monitor both versions and compare results. Are response times comparable? Are error rates the same? If the shadow version crashes, users don\u0026rsquo;t see it. If it\u0026rsquo;s slower, users don\u0026rsquo;t experience it. You\u0026rsquo;re getting real world validation with zero risk.\nShadow deployments are ideal when you need high confidence before switching, when you want to validate against production scale and real data patterns, or when you\u0026rsquo;re making subtle changes that need validation under actual conditions.\nThe complexity comes in traffic duplication, ensuring the shadow system can\u0026rsquo;t write to production systems or trigger side effects, and comparing two streams of data for discrepancies. You also need solid monitoring to surface what the shadow version is doing.\nWhat Are Feature Flags and How Are They Used? #\rFeature flags (sometimes called feature toggles) are perhaps the most underrated tool in modern DevOps. They\u0026rsquo;re simple: a piece of code that says \u0026ldquo;if this flag is on, do this. If it\u0026rsquo;s off, do that.\u0026rdquo;\nThe magic is decoupling deployment from release. You deploy code to production with a feature flag turned off. The code is there, the feature doesn\u0026rsquo;t run. No users see it. You can then flip the flag on at 2 AM for your internal team, watch it work, and gradually enable it for users without redeploying anything.\nFeature flags enable:\nProgressive rollouts: Release to 10% of users, then 50%, then 100%, all with a flag toggle.\nInstant rollbacks: A feature causing problems? Flip the flag off and it stops running immediately. Much faster than a code rollback and redeployment.\nAB testing: Run two versions of a feature for different user segments and measure which performs better.\nReducing deployment risk: You can deploy constantly without releasing constantly. Feature flags separate these concerns.\nSafe development: Teams can merge work in progress to the main branch with flags off, reducing merge conflicts and enabling continuous integration.\nCommon uses look like this:\nif feature_flag(\u0026#39;new_checkout_flow\u0026#39;): # Use the new checkout experience return new_checkout() else: # Fall back to proven flow return legacy_checkout() You typically use a feature flag service like LaunchDarkly, Unleash, or even a simple database. These let you control which users see which flags without code changes. You can target by user ID, geography, user cohort, or whatever criteria matter.\nThe best part? Feature flags work alongside any other strategy. You can use feature flags with rolling deployments, canary releases, or blue green switching. They make everything safer by giving you a quick kill switch.\nPutting It All Together #\rStrategy Downtime Speed Complexity Best For Risk Level Recreate Yes Fast Low Scheduled maintenance, breaking schema changes, non production High Rolling No Slow Medium Most applications, default choice Low Canary No Slow High High risk features, real world validation needed Low Blue/Green No Fast High Fastest rollbacks, stateless applications Low Shadow No Medium High Production scale validation, subtle changes Low Feature Flags No Instant Medium Decouple deployment from release, AB testing, instant rollbacks Very Low Most teams use multiple strategies depending on the change. A risky new payment flow might warrant canary deployment. A bug fix might just be a rolling update with a feature flag kill switch. Database schema changes might need careful planning with recreate deployment.\nThe key is understanding your options and matching them to your risk tolerance and operational constraints.\nHope you found this helpful. Happy hosting!\n","date":"20 October 2025","externalUrl":null,"permalink":"/blogs/blog7-deployment-strategies/","section":"","summary":"","title":"Choosing the Right Deployment Strategy: A DevOps Perspective","type":"blogs"},{"content":"Hello world! I recently decided to deploy Nextcloud on my home server using Kubernetes. I love self-hosting, and using the official Helm chart seemed like the cleanest way to go.\nEverything seemed to install perfectly, but the moment I tried to log in, I hit a wall. I would type my password, click \u0026ldquo;Log in,\u0026rdquo; andâ€¦ nothing. The button just kept spinning. If I refreshed the page, Iâ€™d get stuck in a weird redirect loop or see an \u0026ldquo;Untrusted Domain\u0026rdquo; error. It was driving me crazy!\nAfter a lot of digging and debugging, I figured out it was a reverse proxy issue. Nextcloud didn\u0026rsquo;t realize it was sitting behind an ingress controller, so it was getting confused about http vs https.\nIf you are facing the same thing, donâ€™t worry, Iâ€™ve got the fix right here. Here is how I got it working.\nStep 1: Getting the Official Charts #\rFirst things first, I added the official Nextcloud repository. I prefer using the official sources rather than third-party ones just to stay close to the developers\u0026rsquo; updates.\nhelm repo add nextcloud https://nextcloud.github.io/helm/ helm repo update Step 2: The Problem I Ran Into #\rHere is exactly what was happening to me. I deployed the standard chart, and Nextcloud was running, but it didn\u0026rsquo;t know it was behind an Ingress controller.\nBecause my SSL termination happens at the Ingress level (and not inside the Nextcloud pod itself), Nextcloud thought I was trying to access it via insecure HTTP. It kept trying to redirect me to internal IPs or insecure URLs, and my browser just blocked it.\nStep 3: The Config That Fixed It #\rTo fix this, I had to explicitly tell Nextcloud: \u0026ldquo;Hey, trust the proxy, and pretend you are always on HTTPS.\u0026rdquo;\nI created a values.yaml file and added a specific configs section. This acts like a patch that injects a PHP configuration file right into Nextcloud.\nHere is the exact custom config I used.\n\u0026gt; Note: You\u0026rsquo;ll need to replace nextcloud-mysite.duckdns.org with your actual domain.\nnextcloud: host: nextcloud-mysite.duckdns.org # This is the magic part that fixed my login loop! configs: proxy.config.php: |- \u0026lt;?php $CONFIG = array ( // Trust the Ingress Controller IPs // I added these ranges to cover standard K8s Pod networks \u0026#39;trusted_proxies\u0026#39; =\u0026gt; array( 0 =\u0026gt; \u0026#39;10.0.0.0/8\u0026#39;, 1 =\u0026gt; \u0026#39;172.16.0.0/12\u0026#39;, 2 =\u0026gt; \u0026#39;192.168.0.0/16\u0026#39;, ), // Force Nextcloud to use my external domain for redirects \u0026#39;overwritehost\u0026#39; =\u0026gt; \u0026#39;nextcloud-mysite.duckdns.org\u0026#39;, // Force Nextcloud to generate HTTPS links \u0026#39;overwriteprotocol\u0026#39; =\u0026gt; \u0026#39;https\u0026#39;, // Ensure CLI commands (like cron jobs) use the correct URL \u0026#39;overwrite.cli.url\u0026#39; =\u0026gt; \u0026#39;https://nextcloud-mysite.duckdns.org\u0026#39;, ); ingress: enabled: true className: nginx annotations: kubernetes.io/ingress.class: nginx hosts: - host: nextcloud-mysite.duckdns.org paths: - path: / pathType: Prefix tls: [] Step 4: Launching It #\rOnce I had that file saved as nextcloud-values.yaml, I simply installed the chart pointing to it:\nhelm install nextcloud nextcloud/nextcloud -f nextcloud-values.yaml I waited a minute for the pods to spin up:\nkubectl get pods -w And that was it! I went to my URL, and the login worked instantly. No more spinning wheel, no more loops.\nA Quick Troubleshooting Tip #\rIf you use this config and still see an \u0026ldquo;Untrusted Domain\u0026rdquo; error, double-check that the overwritehost line in the YAML matches the URL in your browser exactly. It\u0026rsquo;s super picky about that!\nI hope this quick guide saves you the hours of troubleshooting it took me.\nHappy hosting!\n","date":"7 October 2025","externalUrl":null,"permalink":"/blogs/blog6-nextcloud/","section":"","summary":"","title":"Nextcloud redirect issue fixed!","type":"blogs"},{"content":"Container images are the foundation of modern DevOps practices. They package your application with all its dependencies, making deployment consistent across different environments. However, not all container images are created equal. This guide walks you through building container images the right way, starting with the basics and progressing to advanced best practices.\nWhy Container Image Quality Matters #\rBefore diving into the technical details, let\u0026rsquo;s understand why crafting better container images is essential. Poor container images lead to larger deployments, slower pull times, increased storage costs, and potential security vulnerabilities. By following industry best practices, you can create images that are lean, secure, and efficient.\nStarting Simple: Your First Container Image #\rLet\u0026rsquo;s begin with a basic Dockerfile. This is the simplest possible approach to containerizing an application.\nFROM python:3.9 WORKDIR /app COPY . . RUN pip install -r requirements.txt CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] What\u0026rsquo;s happening here:\nThe FROM instruction sets the base image. We\u0026rsquo;re using Python 3.9, which comes pre-installed with Python and pip. WORKDIR creates a working directory inside the container. COPY brings your application files into the container. RUN executes the pip install command to install dependencies. Finally, CMD specifies what command runs when the container starts.\nThis works, but it\u0026rsquo;s not optimized. The resulting image is large, and every time you rebuild it, you\u0026rsquo;re installing all dependencies from scratch.\nBuilding Better: Introducing Layer Caching #\rDocker builds images in layers. Each instruction creates a new layer. Understanding this helps you write faster, more efficient Dockerfiles.\nFROM python:3.9 WORKDIR /app COPY requirements.txt . RUN pip install -r requirements.txt COPY . . CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] What changed:\nWe separated the COPY instructions. Now we copy requirements.txt first and run pip install before copying the application code. Why? Dependencies change less frequently than application code. By ordering instructions this way, Docker caches the dependency layer. When you rebuild after changing your code, Docker reuses the cached dependency layer instead of reinstalling everything.\nThis simple change can cut build times in half or more.\nGoing Minimal: Using Slim and Alpine Base Images #\rBase images contain the operating system and preinstalled tools. The standard Python image is quite large. Slimmer alternatives exist.\nFROM python:3.9-alpine WORKDIR /app COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt COPY . . CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] What\u0026rsquo;s new:\nalpine is a minimal Linux distribution. A Python 3.9-alpine image is just a few megabytes, compared to the standard image which is hundreds of megabytes. The --no-cache-dir flag tells pip not to store the cache after installation, reducing image size further.\nTrade-off: Alpine uses musl libc instead of glibc. Some applications built for glibc may not work on Alpine. If you encounter compatibility issues, use python:3.9-slim instead. It\u0026rsquo;s still smaller than the standard image but includes more tools.\nLayering Strategy: Multi-stage Builds #\rMulti-stage builds allow you to use multiple base images in one Dockerfile. This is powerful for reducing final image size, especially for compiled languages.\nFROM python:3.9-alpine AS builder WORKDIR /app COPY requirements.txt . RUN pip install --user --no-cache-dir -r requirements.txt FROM python:3.9-alpine WORKDIR /app COPY --from=builder /root/.local /root/.local ENV PATH=/root/.local/bin:$PATH COPY . . CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] What\u0026rsquo;s happening:\nThe first stage is named builder. It installs Python packages into /root/.local using the --user flag. The second stage starts fresh from a clean Alpine image. COPY --from=builder copies only the installed packages from the builder stage, skipping the entire pip cache and build artifacts. We set the PATH environment variable to include the copied packages.\nThe builder stage is discarded after the build completes. Your final image contains only what\u0026rsquo;s necessary to run the application, making it significantly smaller.\nSecurity First: Non-root Users #\rRunning processes as root inside containers is a security risk. A compromised application could potentially affect the host system. Create a dedicated user for your application.\nFROM python:3.9-alpine RUN addgroup -g 1001 appgroup \u0026amp;\u0026amp; \\ adduser -D -u 1001 -G appgroup appuser WORKDIR /app COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt COPY --chown=appuser:appgroup . . USER appuser CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] What\u0026rsquo;s new:\naddgroup and adduser create a system user and group with specific UIDs and GIDs. --chown=appuser:appgroup ensures the copied files are owned by our non-root user. USER appuser switches to this user before running the command. Now your application runs with minimal privileges.\nReducing Attack Surface: Minimal Dependencies #\rScan your Dockerfile for unnecessary dependencies. Every tool installed is a potential vulnerability.\nFROM python:3.9-alpine RUN apk add --no-cache curl RUN addgroup -g 1001 appgroup \u0026amp;\u0026amp; \\ adduser -D -u 1001 -G appgroup appuser WORKDIR /app COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt COPY --chown=appuser:appgroup . . USER appuser HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\ CMD curl -f http://localhost:8000/health || exit 1 CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] What\u0026rsquo;s new:\nHEALTHCHECK tells Docker how to verify that your container is still working properly. If the health check fails, Docker can restart the container automatically. This improves reliability in production.\napk add --no-cache curl installs curl for health checks but skips Alpine\u0026rsquo;s package cache, keeping the image lean.\nAdvanced: Reducing Secrets and Build Context #\rSensitive data like API keys should never end up in your image. Use Docker build secrets.\nFROM python:3.9-alpine RUN addgroup -g 1001 appgroup \u0026amp;\u0026amp; \\ adduser -D -u 1001 -G appgroup appuser WORKDIR /app COPY requirements.txt . RUN --mount=type=secret,id=pypi_token \\ pip install --no-cache-dir -r requirements.txt COPY --chown=appuser:appgroup . . USER appuser CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] What\u0026rsquo;s new:\nRUN --mount=type=secret mounts a secret file during the build. The secret is available inside that specific RUN command but isn\u0026rsquo;t saved in the image layers. This keeps credentials out of your container images entirely.\nBuild it with:\ndocker build --secret pypi_token=/path/to/token.txt -t myapp . Production-Ready: Complete Best Practices Example #\rHere\u0026rsquo;s a comprehensive Dockerfile incorporating all the principles discussed.\n# Build stage FROM python:3.9-alpine AS builder WORKDIR /build COPY requirements.txt . RUN pip install --user --no-cache-dir -r requirements.txt # Runtime stage FROM python:3.9-alpine ARG VERSION=unknown ARG BUILD_DATE LABEL version=${VERSION} LABEL build.date=${BUILD_DATE} RUN apk add --no-cache curl RUN addgroup -g 1001 appgroup \u0026amp;\u0026amp; \\ adduser -D -u 1001 -G appgroup appuser WORKDIR /app COPY --from=builder /root/.local /root/.local ENV PATH=/root/.local/bin:$PATH \\ PYTHONUNBUFFERED=1 COPY --chown=appuser:appgroup . . USER appuser EXPOSE 8000 HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\ CMD curl -f http://localhost:8000/health || exit 1 CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] Key additions:\nARG defines build arguments that you can pass at build time with --build-arg. LABEL adds metadata to your image for tracking versions and build information. EXPOSE documents which ports your application uses (it doesn\u0026rsquo;t actually publish ports, but it documents intent). PYTHONUNBUFFERED=1 ensures Python outputs logs immediately instead of buffering them, which is essential for container logging.\nBuild with:\ndocker build \\ --build-arg VERSION=1.0.0 \\ --build-arg BUILD_DATE=$(date -u +\u0026#39;%Y-%m-%dT%H:%M:%SZ\u0026#39;) \\ -t myapp:1.0.0 . Image Scanning and Validation #\rBefore pushing images to production, always scan them for vulnerabilities.\ndocker scan myapp:1.0.0 Or use Trivy, a popular open-source scanner:\ntrivy image myapp:1.0.0 These tools check for known vulnerabilities in base images and dependencies, helping you catch issues before they reach production.\nKey Takeaways #\rBuilding better container images is a journey, not a destination. Start with the fundamentals: understand layer caching and use appropriate base images. Progress toward security by running as non-root users and removing unnecessary dependencies. Finally, implement advanced techniques like multi-stage builds, secrets management, and image scanning.\nEach optimization might seem small, but together they create images that are faster to build, smaller to distribute, more secure to run, and easier to maintain. As you continue working with containers, these practices will become second nature, allowing you to focus on what matters most: your application.\nStart applying these techniques today, and watch your deployment pipelines become more efficient, secure, and reliable.\n","date":"3 October 2025","externalUrl":null,"permalink":"/blogs/blog5-docker-image-best-practices/","section":"","summary":"","title":"Building Production-Ready Container Images","type":"blogs"},{"content":"Docker has become the foundation of modern application deployment. Whether you\u0026rsquo;re containerizing applications, managing development environments, or orchestrating microservices, knowing the right Docker commands can save you hours of troubleshooting. This comprehensive guide walks you through the most useful Docker commands, starting from the basics and progressing to more advanced operations.\nGetting Started: Basic Commands #\rCheck Docker Installation #\rBefore running any Docker commands, verify that Docker is installed and running on your system.\ndocker --version docker info docker run hello-world The docker --version command displays the installed Docker version, docker info shows system-wide information about Docker, and docker run hello-world confirms that Docker daemon is running properly.\nRunning Your First Container #\rThe most fundamental Docker command is docker run. This command creates and starts a new container from an image.\ndocker run nginx This pulls the nginx image from Docker Hub and runs it. The container will run in the foreground by default. To run it in the background (detached mode), use the -d flag:\ndocker run -d nginx docker run -d --name my-nginx nginx The --name flag assigns a custom name to your container, making it easier to manage.\nListing and Managing Containers #\rSee all running containers:\ndocker ps docker container ls To view all containers, including stopped ones:\ndocker ps -a docker ps -aq The -q flag shows only container IDs, which is useful for scripting and batch operations.\nIntermediate Operations: Working with Containers #\rStarting, Stopping, and Removing Containers #\rStop a running container:\ndocker stop \u0026lt;container_id\u0026gt; docker kill \u0026lt;container_id\u0026gt; The stop command sends a SIGTERM signal followed by SIGKILL after a grace period, while kill immediately sends SIGKILL.\nStart a stopped container:\ndocker start \u0026lt;container_id\u0026gt; docker restart \u0026lt;container_id\u0026gt; Remove a stopped container:\ndocker rm \u0026lt;container_id\u0026gt; docker rm -f \u0026lt;container_id\u0026gt; The -f flag forces removal of running containers.\nRemove all stopped containers at once:\ndocker container prune docker rm $(docker ps -aq) Accessing Container Logs #\rView the output from a container:\ndocker logs \u0026lt;container_id\u0026gt; docker logs --tail 50 \u0026lt;container_id\u0026gt; Follow logs in real-time (like tail -f):\ndocker logs -f \u0026lt;container_id\u0026gt; docker logs -f --since 10m \u0026lt;container_id\u0026gt; The --since flag shows logs from a specific time period, and --tail limits the number of lines displayed.\nExecuting Commands Inside Containers #\rRun a command inside a container:\ndocker exec \u0026lt;container_id\u0026gt; ls -la docker exec \u0026lt;container_id\u0026gt; cat /etc/hosts To access the container\u0026rsquo;s shell interactively:\ndocker exec -it \u0026lt;container_id\u0026gt; /bin/bash docker exec -it \u0026lt;container_id\u0026gt; sh The -i flag keeps STDIN open even if not attached, and -t allocates a pseudo-terminal. Together, -it allows you to interact with the container shell just as if you were SSH\u0026rsquo;d into a remote machine.\nInspecting Containers #\rGet detailed information about a container:\ndocker inspect \u0026lt;container_id\u0026gt; docker inspect --format=\u0026#39;{{.State.Status}}\u0026#39; \u0026lt;container_id\u0026gt; The --format flag allows you to extract specific fields using Go templates. This outputs comprehensive JSON data about the container\u0026rsquo;s configuration, network settings, volumes, and environment variables.\nWorking with Images #\rPulling and Searching Images #\rPull an image from Docker Hub:\ndocker pull ubuntu:22.04 docker pull nginx:latest Search for images on Docker Hub:\ndocker search nginx docker search --filter stars=100 nginx Building Your Own Images #\rCreate a Dockerfile in your project directory:\nFROM ubuntu:22.04 RUN apt-get update \u0026amp;\u0026amp; apt-get install -y python3 COPY . /app WORKDIR /app EXPOSE 8000 CMD [\u0026#34;python3\u0026#34;, \u0026#34;app.py\u0026#34;] Build an image from this Dockerfile:\ndocker build -t my-app:1.0 . docker build -t my-app:1.0 --no-cache . The -t flag tags the image with a name and version. The . specifies the build context (current directory). The --no-cache flag ensures a fresh build without using cached layers.\nListing and Managing Images #\rView all images on your system:\ndocker images docker image ls View image history and layers:\ndocker history \u0026lt;image_id\u0026gt; Remove an image:\ndocker rmi \u0026lt;image_id\u0026gt; docker image rm \u0026lt;image_id\u0026gt; Remove all unused images:\ndocker image prune docker image prune -a The -a flag removes all unused images, not just dangling ones.\nTagging and Pushing Images #\rTag an image for a registry:\ndocker tag my-app:1.0 username/my-app:1.0 docker tag my-app:1.0 myregistry.com/my-app:latest Login to a registry:\ndocker login docker login myregistry.com Push an image to a registry:\ndocker push username/my-app:1.0 This allows you to store images in Docker Hub, private registries, or cloud repositories for sharing and deployment.\nAdvanced Operations #\rPort Mapping and Networking #\rRun a container and expose ports:\ndocker run -d -p 8080:80 nginx docker run -d -p 127.0.0.1:8080:80 nginx This maps port 8080 on your host machine to port 80 inside the container. Now, accessing http://localhost:8080 will reach the nginx container. The second command binds only to localhost for security.\nView port mappings:\ndocker port \u0026lt;container_id\u0026gt; Volume Mounting and Data Persistence #\rMount a directory from your host machine into the container:\ndocker run -d -v /host/path:/container/path nginx docker run -d -v my-volume:/data nginx This creates a persistent connection. Changes made in either location are reflected in the other. Volumes are essential for data persistence and local development workflows.\nCreate and manage named volumes:\ndocker volume create my-volume docker volume ls docker volume inspect my-volume docker volume rm my-volume docker volume prune Copy files between host and container:\ndocker cp myfile.txt \u0026lt;container_id\u0026gt;:/app/ docker cp \u0026lt;container_id\u0026gt;:/app/output.log ./ Running Containers with Environment Variables #\rPass environment variables to a container:\ndocker run -d -e DATABASE_URL=postgres://db:5432 my-app docker run -d --env-file .env my-app Use the -e flag for single variables or --env-file to load multiple variables from a file. This is crucial for configuration management without modifying the container image.\nResource Limits and Constraints #\rLimit container resources:\ndocker run -d --memory=\u0026#34;512m\u0026#34; --cpus=\u0026#34;1.5\u0026#34; nginx docker run -d --memory=\u0026#34;1g\u0026#34; --memory-swap=\u0026#34;2g\u0026#34; nginx These flags prevent containers from consuming all system resources.\nMonitoring Container Performance #\rView real-time resource usage statistics:\ndocker stats docker stats \u0026lt;container_id\u0026gt; docker stats --no-stream The docker stats command shows CPU, memory, network I/O, and disk I/O for running containers. The --no-stream flag displays a single snapshot instead of continuous updates.\nDisplay running processes inside a container:\ndocker top \u0026lt;container_id\u0026gt; docker top \u0026lt;container_id\u0026gt; aux This shows process information similar to the Linux top command, but specific to the container.\nInspecting Filesystem Changes #\rView filesystem changes in a container:\ndocker diff \u0026lt;container_id\u0026gt; This command shows files and directories that have been added (A), modified (C), or deleted (D) since the container was created. It\u0026rsquo;s invaluable for debugging and understanding what changes occurred during runtime.\nSaving and Loading Images #\rSave an image to a tar archive:\ndocker save -o myimage.tar my-app:1.0 docker save my-app:1.0 \u0026gt; myimage.tar Load an image from a tar archive:\ndocker load -i myimage.tar docker load \u0026lt; myimage.tar The save and load commands preserve the entire image with all layers, tags, and metadata.\nExporting and Importing Containers #\rExport a container\u0026rsquo;s filesystem:\ndocker export \u0026lt;container_id\u0026gt; -o mycontainer.tar docker export \u0026lt;container_id\u0026gt; \u0026gt; mycontainer.tar Import a container filesystem as an image:\ndocker import mycontainer.tar my-new-image:1.0 cat mycontainer.tar | docker import - my-new-image:1.0 Unlike save/load, the export/import commands flatten the container into a single layer without history or metadata.\nCreating Images from Containers #\rCommit changes in a container to a new image:\ndocker commit \u0026lt;container_id\u0026gt; my-new-image:1.0 docker commit -m \u0026#34;Added nginx config\u0026#34; -a \u0026#34;John Doe\u0026#34; \u0026lt;container_id\u0026gt; my-new-image:1.0 The -m flag adds a commit message, and -a specifies the author. This is useful for creating images from modified containers, though Dockerfiles are preferred for reproducibility.\nDocker Networking #\rList available networks:\ndocker network ls Create a custom network:\ndocker network create my-network docker network create --driver bridge --subnet 192.168.1.0/24 my-network Connect and disconnect containers from networks:\ndocker network connect my-network \u0026lt;container_id\u0026gt; docker network disconnect my-network \u0026lt;container_id\u0026gt; Inspect network details:\ndocker network inspect my-network Remove networks:\ndocker network rm my-network docker network prune Run containers on specific networks:\ndocker run -d --network my-network --name web nginx docker run -d --network my-network --name db postgres Containers on the same custom network can communicate using container names as hostnames.\nDocker Compose for Multi-Container Applications #\rFor applications requiring multiple services, Docker Compose simplifies orchestration. Create a docker-compose.yml:\nversion: \u0026#39;3\u0026#39; services: web: image: nginx ports: - \u0026#34;8080:80\u0026#34; networks: - app-network depends_on: - db db: image: postgres environment: POSTGRES_PASSWORD: secret networks: - app-network volumes: - db-data:/var/lib/postgresql/data networks: app-network: volumes: db-data: Essential Docker Compose commands:\ndocker-compose up -d docker-compose down docker-compose ps docker-compose logs -f docker-compose exec web bash docker-compose build docker-compose restart docker-compose up --scale web=3 System Maintenance and Cleanup #\rCleaning Up Resources #\rDocker can quickly consume disk space. Clean up unused resources:\ndocker system prune docker system prune -a docker system prune -a --volumes The first command removes all stopped containers, unused networks, and dangling images. The -a flag also removes unused images, and --volumes removes unused volumes.\nView Docker disk usage:\ndocker system df docker system df -v Remove specific resource types:\ndocker container prune docker image prune docker volume prune docker network prune Pausing and Unpausing Containers #\rPause all processes in a container:\ndocker pause \u0026lt;container_id\u0026gt; docker unpause \u0026lt;container_id\u0026gt; This freezes the container without stopping it, useful for temporary resource management.\nRenaming Containers #\rRename a container:\ndocker rename old-name new-name Viewing Events #\rMonitor Docker daemon events in real-time:\ndocker events docker events --since 1h docker events --filter type=container This displays real-time information about container lifecycle events, network changes, and image operations.\nWaiting for Container Exit #\rBlock until a container stops and print its exit code:\ndocker wait \u0026lt;container_id\u0026gt; This is useful in scripts where you need to wait for a container to complete before proceeding.\nQuick Reference Table #\rCommand Category Common Commands Container Lifecycle docker run, docker start, docker stop, docker restart, docker kill, docker rm Container Information docker ps, docker logs, docker top, docker stats, docker inspect, docker diff Image Management docker build, docker pull, docker push, docker images, docker rmi, docker tag Data Management docker volume create, docker volume ls, docker cp, docker commit Networking docker network create, docker network connect, docker network ls, docker network inspect System Maintenance docker system prune, docker system df, docker events Import/Export docker save, docker load, docker export, docker import Conclusion #\rDocker commands follow a logical progression from basic container management to advanced multi-container orchestration. Mastering these commandsâ€”from docker run and docker ps for everyday operations to docker exec, docker stats, and docker network for complex workflowsâ€”will significantly improve your DevOps efficiency.\nStart with the basic commands like running and listing containers, gradually incorporate intermediate operations such as volume mounting and port mapping into your workflow, and progressively explore advanced features like networking, resource monitoring, and Docker Compose as your needs grow. Understanding commands like docker diff for filesystem inspection, docker top for process monitoring, and docker save/docker load for image portability will give you powerful debugging and deployment capabilities.\nRemember, the Docker documentation and docker \u0026lt;command\u0026gt; --help are always available when you need quick reference material. With this comprehensive cheatsheet at your disposal, you\u0026rsquo;re well-equipped to handle containerized applications efficiently in any DevOps environment. Happy containerizing!\n","date":"18 September 2025","externalUrl":null,"permalink":"/blogs/blog4-docker-cheatsheet/","section":"","summary":"","title":"Docker Cheatsheet: Essential Commands for DevOps Engineers","type":"blogs"},{"content":" Kubernetes is great at managing containers, but getting traffic into and between your pods can feel confusing at first. That\u0026rsquo;s where Services come in. A Service is a stable way to expose your applications and manage how pods talk to each other. Let\u0026rsquo;s break down the different types of services and when to use each one.\nWhat is a Kubernetes Service? #\rBefore diving into types, it helps to understand what a Service actually does. Pods in Kubernetes are temporaryâ€”they can be created and destroyed frequently. A Service provides a stable IP address and DNS name that acts as a single entry point to reach a group of pods, even as individual pods come and go.\nThink of it like a load balancer sitting in front of your pods. Requests come to the Service, and it routes them to available pods behind it.\nClusterIP Service #\rClusterIP is the default service type in Kubernetes, and it\u0026rsquo;s the simplest one to understand.\nA ClusterIP service creates an internal IP address that\u0026rsquo;s only accessible from within your Kubernetes cluster. No external traffic can reach it directly. This is perfect for internal communication between your microservices.\nUse cases:\nBackend APIs that only your frontend needs to talk to Database services used by multiple applications Internal tools and monitoring services Microservice-to-microservice communication Example:\napiVersion: v1 kind: Service metadata: name: my-app spec: type: ClusterIP selector: app: my-app ports: - protocol: TCP port: 80 targetPort: 8080 NodePort Service #\rNodePort is the next step up. It exposes your service on a specific port across every node in your cluster. This makes your application accessible from outside the cluster using the node\u0026rsquo;s IP address and the assigned port (usually in the 30000-32767 range).\nUse cases:\nDevelopment and testing environments Temporary external access without setting up a proper ingress Services that need direct port access Non-HTTP protocols that ingress can\u0026rsquo;t handle Example:\napiVersion: v1 kind: Service metadata: name: my-app spec: type: NodePort selector: app: my-app ports: - protocol: TCP port: 80 targetPort: 8080 nodePort: 30080 With this configuration, you could access your app at http://node-ip:30080.\nLoadBalancer Service #\rLoadBalancer is the cloud-native approach. When you create a LoadBalancer service, Kubernetes works with your cloud provider (like AWS, Azure, or Google Cloud) to provision an actual load balancer. This gives you a real external IP address that clients can use.\nUse cases:\nProduction applications that need external access Web applications serving end users APIs that external systems need to call When you need automatic SSL certificate management through your cloud provider Example:\napiVersion: v1 kind: Service metadata: name: my-app spec: type: LoadBalancer selector: app: my-app ports: - protocol: TCP port: 80 targetPort: 8080 ExternalName Service #\rExternalName is the odd one out. It doesn\u0026rsquo;t route to pods at all. Instead, it\u0026rsquo;s a DNS alias that maps a Kubernetes service name to an external DNS name.\nUse cases:\nConnecting to external databases (like managed databases outside your cluster) Integrating with third-party APIs Gradually migrating services from outside Kubernetes into your cluster Abstracting external service locations Example:\napiVersion: v1 kind: Service metadata: name: external-db spec: type: ExternalName externalName: my-database.example.com port: 5432 Now pods inside your cluster can reach it using external-db.default.svc.cluster.local.\nBonus: Ingress (Not Technically a Service) #\rWhile not a service type itself, Ingress is worth mentioning. It\u0026rsquo;s a more advanced way to expose HTTP/HTTPS services to the outside world. Ingress is better than LoadBalancer for most production scenarios because it gives you:\nMultiple applications on the same IP Path-based routing Virtual hosting SSL termination For production web applications, Ingress is usually your best choice over LoadBalancer.\nQuick Comparison #\rService Type Scope Port Range Best For ClusterIP Internal only Any Internal communication NodePort Cluster nodes 30000-32767 Development/testing LoadBalancer External Any Production apps ExternalName External DNS N/A External services Conclusion #\rChoosing the right service type depends on how you want traffic to reach your application. Start with ClusterIP for most of your internal services. Use NodePort during development. For production, prefer LoadBalancer for simple cases or Ingress for more advanced traffic management. And remember ExternalName when you need to reference services outside your cluster. Understanding these options gives you the flexibility to design reliable, scalable Kubernetes applications.\n","date":"17 September 2025","externalUrl":null,"permalink":"/blogs/blog3-k8s-services/","section":"","summary":"","title":"Understanding Kubernetes Services: Types and Use Cases","type":"blogs"},{"content":"Hey there! If you\u0026rsquo;ve been wondering what all the fuss is about GitHub Actions, you\u0026rsquo;re in the right place. I\u0026rsquo;m going to walk you through this amazing feature that completely changed how I handle automation in my projects.\nWhat Exactly is GitHub Actions? #\rSo here\u0026rsquo;s the thing: GitHub Actions is basically your personal automation buddy that lives right inside your GitHub repository. It\u0026rsquo;s a feature that lets you write custom automated workflows to run whenever something happens in your repo. Think of it as a robot that does your bidding whenever you push code, open a pull request, or even on a schedule you set up.\nI remember when I first started using GitHub Actions, I was coming from managing Jenkins servers and writing complicated pipeline scripts. GitHub Actions felt like a breath of fresh air because everything was integrated directly into my repository. No extra servers to maintain, no complex setup. Just pure automation living alongside your code.\nThe Key Concepts You Need to Know #\rBefore we dive into actually building workflows, let me break down the main concepts. Don\u0026rsquo;t worry if it seems like a lot at first, I\u0026rsquo;ll explain each piece and it\u0026rsquo;ll all click together.\nWorkflows #\rA workflow is basically a complete automated process. You define it using a YAML file and store it in your repo under the .github/workflows directory. Think of it as your blueprint for automation. The workflow contains all the instructions for what should happen and when.\nEvents #\rEvents are the triggers that kick off your workflow. Something happens in your GitHub repository or even outside of it, and boom, your workflow starts running. Some common events I use all the time are:\npush: Triggers when someone pushes code to your repository pull_request: Triggers when someone opens or updates a pull request schedule: Lets you run workflows on a schedule, like a cron job workflow_dispatch: Allows you to manually trigger a workflow from the UI You can get pretty specific with these triggers too. For example, you can say \u0026ldquo;only run this when someone pushes to the main branch\u0026rdquo; or \u0026ldquo;only when certain files change\u0026rdquo;. That\u0026rsquo;s the kind of control I love.\nJobs #\rJobs are collections of steps, and here\u0026rsquo;s the cool part: they run in parallel by default. So if you have multiple jobs, they\u0026rsquo;ll all start at the same time unless you tell them otherwise. You can also make jobs wait for other jobs to finish using the needs keyword. This is super useful when you have dependencies between different parts of your workflow.\nSteps #\rSteps are the individual tasks within a job. Each step is a single command or action that gets executed in order. Steps can either run a script you write (using bash, Python, whatever you want) or use a pre built action from the GitHub Marketplace.\nActions #\rActions are reusable blocks of code that do specific things. GitHub provides a bunch of official actions that are super helpful, and the community has created thousands more. You can find actions for setting up Node.js, running tests, deploying to cloud providers, or basically anything else you can imagine.\nI use the actions/checkout action in basically every workflow because it checks out your code so your job can access it. It\u0026rsquo;s one of those essential actions that\u0026rsquo;s just always there.\nRunners #\rA runner is the machine that actually executes your workflow. GitHub provides hosted runners (ubuntu, windows, macos) that you can use for free if your repository is public. If you need more control or privacy, you can set up self hosted runners on your own infrastructure. For most of my projects, the GitHub hosted runners work perfectly fine.\nA Real World Example: CI with Docker and Security Scanning #\rLet me show you a workflow that I actually use in production. This one handles building and pushing Docker images while running security scans. It\u0026rsquo;s what I use with GitOps deployments, and it covers a lot of real world concerns like testing, code quality, security, and containerization.\nFirst, create this file in your repo:\n.github/ workflows/ ci-build-push.yml Here\u0026rsquo;s the complete workflow:\nname: CI - Build \u0026amp; Push Image (GitOps) # Descriptive workflow name on: # Define when this workflow runs push: # Trigger on code pushes branches: - main # Only for the main branch pull_request: # Also run on pull requests jobs: build: # Single job that does everything runs-on: ubuntu-latest # Use Ubuntu runner steps: - uses: actions/checkout@v4 # Clone your repository code - uses: actions/setup-node@v4 # Install Node.js runtime with: node-version: 20 # Use Node version 20 - run: npm ci # Clean install of dependencies (better than npm install for CI) - run: npm test # Run all unit tests to catch bugs early - uses: SonarSource/sonarqube-scan-action@v2 # SAST: Static Application Security Testing env: SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }} # Your SonarQube security token (keep it secret!) SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }} # Your SonarQube server URL - uses: aquasecurity/trivy-action@v0.24.0 # Scan for filesystem vulnerabilities with: scan-type: fs # Scan the filesystem for vulnerabilities severity: HIGH,CRITICAL # Only fail on high and critical severity issues exit-code: 1 # Fail the job if critical issues are found - uses: docker/login-action@v3 # Login to Docker registry (Docker Hub in this case) with: username: ${{ secrets.DOCKERHUB_USERNAME }} # Your Docker Hub username from secrets password: ${{ secrets.DOCKERHUB_TOKEN }} # Your Docker Hub token from secrets - uses: docker/build-push-action@v5 # Build and push Docker image with: push: true # Push the built image to the registry tags: | # Tag the image with multiple tags username/app:${{ github.sha }} # Use commit SHA as tag for unique identification This workflow is a complete CI/CD pipeline. Here\u0026rsquo;s what happens step by step: first, it checks out your code and sets up Node.js. Then it installs dependencies and runs your tests. Next, it does a SAST scan using SonarQube to check for code quality issues. After that, Trivy scans your filesystem for known vulnerabilities. If everything passes, it logs into Docker Hub and builds your Docker image, tagging it with the commit SHA. This makes it GitOps friendly because each image is uniquely identifiable by its commit.\nThe really powerful part here is using ${{ github.sha }} as the image tag. This means each commit gets a unique Docker image, which is perfect for GitOps workflows where you can reference exact versions in your deployment manifests.\nUsing Secrets to Keep Things Safe #\rGitHub helps you with Secrets. You can set them in your repository settings under \u0026ldquo;Secrets and variables\u0026rdquo;. Then you can access them in your workflow like this:\n- name: Deploy to production # Step name run: deploy.sh # Run deployment script env: # Define environment variables API_KEY: ${{ secrets.MY_API_KEY }} # Reference secret API key WEBHOOK_URL: ${{ secrets.WEBHOOK_URL }} # Reference secret webhook URL The secrets are masked in the logs too, so even if someone has access to your workflow runs, they can\u0026rsquo;t see the actual values.\nViewing Your Workflow Results #\rAfter you\u0026rsquo;ve pushed your workflow file and triggered it, you can see what\u0026rsquo;s happening by going to your repository and clicking on the \u0026ldquo;Actions\u0026rdquo; tab. You\u0026rsquo;ll see all your past workflow runs there. Click on one and you can see the detailed logs of what happened at each step.\nTips from My Experience #\rHere are some things I\u0026rsquo;ve learned along the way:\nUse the GitHub Marketplace: Before you write custom scripts, check the Marketplace. Someone probably already created an action for what you need.\nSpecify the Action Versions: Always pin actions (@v4, @v2, or commit SHA) to avoid breaking changes.\nResource Limits: GitHub-hosted runners have CPU, memory, and runtime constraints. For heavy builds, consider self-hosted runners or split jobs into smaller steps.\nUse descriptive step names: When you run a workflow, those step names show up in the logs. Make them clear so you (and your team) know what\u0026rsquo;s happening.\nExit Codes: Steps with non-zero exit code fail the job. Use exit-code in actions like Trivy intentionally. Use \u0026ldquo;continue-on-error\u0026rdquo; only for optional steps.\nWrapping Up #\rGitHub Actions is honestly one of my favorite tools in the DevOps toolkit. It\u0026rsquo;s powerful, flexible, and integrated directly into the platform you\u0026rsquo;re already using. The fact that you can automate basically anything right from your repository is just amazing.\nWhether you\u0026rsquo;re running tests, building Docker images, deploying applications, or just automating repetitive tasks, GitHub Actions has got you covered. And the best part? For public repositories, it\u0026rsquo;s completely free.\nHope you found this helpful. Happy hosting!\n","date":"29 August 2025","externalUrl":null,"permalink":"/blogs/blog1-github-actions/","section":"","summary":"","title":"Getting Started with GitHub Actions","type":"blogs"},{"content":"If you\u0026rsquo;re working with Kubernetes, you\u0026rsquo;ve probably noticed that managing YAML files across different environments can get messy quickly. That\u0026rsquo;s where Helm comes in. Helm is a package manager for Kubernetes that helps you define, install, and manage applications using something called charts. In this guide, we\u0026rsquo;ll walk through creating your first Helm chart step by step.\nWhat is a Helm Chart? #\rA Helm chart is essentially a collection of files that describe Kubernetes resources. Think of it as a template for your application that you can reuse across multiple environments like development, staging, and production. Instead of maintaining separate YAML files for each environment, you create one chart and customize it with different values.\nPrerequisites #\rBefore we start, make sure you have:\nHelm installed on your machine Access to a Kubernetes cluster Basic understanding of Kubernetes concepts like Deployments and Services Step 1: Create Your First Chart #\rCreating a new Helm chart is straightforward. Open your terminal and run:\nhelm create my-first-chart This command generates a directory called my-first-chart with a standard structure. Let\u0026rsquo;s explore what\u0026rsquo;s inside.\nStep 2: Understanding the Chart Structure #\rWhen you navigate into the newly created directory, you\u0026rsquo;ll see several files and folders:\nmy-first-chart/ â”œâ”€â”€ Chart.yaml â”œâ”€â”€ values.yaml â”œâ”€â”€ charts/ â””â”€â”€ templates/ â”œâ”€â”€ deployment.yaml â”œâ”€â”€ service.yaml â”œâ”€â”€ _helpers.tpl â””â”€â”€ NOTES.txt Here\u0026rsquo;s what each component does:\nChart.yaml: This file contains metadata about your chart like its name, version, and description. It\u0026rsquo;s like the identity card for your Helm chart.\nvalues.yaml: This is where you define default configuration values. These values can be referenced in your templates and easily overridden during installation.\ntemplates/: This directory holds all your Kubernetes manifest files. Helm processes these files through a templating engine before sending them to Kubernetes.\ncharts/: If your chart depends on other charts, they go here.\nStep 3: Customize Chart.yaml #\rOpen the Chart.yaml file and update it with your chart details:\napiVersion: v2 name: my-first-chart description: My very first Helm chart type: application version: 0.1.0 appVersion: \u0026#34;1.0.0\u0026#34; The apiVersion: v2 is used for Helm 3, while type: application indicates this chart deploys an application (as opposed to a library chart).\nStep 4: Define Your Values #\rThe values.yaml file lets you parameterize your templates. Here\u0026rsquo;s a simple example:\nreplicaCount: 2 image: repository: nginx tag: \u0026#34;1.21.0\u0026#34; pullPolicy: IfNotPresent service: type: ClusterIP port: 80 These values will be injected into your templates, making your chart flexible and reusable.\nStep 5: Create Templates #\rNow comes the fun part. Templates are regular Kubernetes YAML files with special template directives. Let\u0026rsquo;s look at a simple deployment template:\napiVersion: apps/v1 kind: Deployment metadata: name: {{ .Release.Name }}-deployment spec: replicas: {{ .Values.replicaCount }} selector: matchLabels: app: {{ .Release.Name }} template: metadata: labels: app: {{ .Release.Name }} spec: containers: - name: {{ .Chart.Name }} image: \u0026#34;{{ .Values.image.repository }}:{{ .Values.image.tag }}\u0026#34; imagePullPolicy: {{ .Values.image.pullPolicy }} ports: - containerPort: 80 Notice the template directives enclosed in double curly braces like {{ .Values.replicaCount }}. These get replaced with actual values from your values.yaml file when you install the chart.\nStep 6: Test Your Chart #\rBefore installing, it\u0026rsquo;s wise to validate your chart. Run this command from your chart directory:\nhelm lint . This checks for errors and formatting issues. To see what Kubernetes manifests will be generated without actually installing anything, use:\nhelm template . You can also do a dry run:\nhelm install --dry-run --debug my-release ./my-first-chart Step 7: Install Your Chart #\rOnce everything looks good, install your chart:\nhelm install my-release ./my-first-chart Here, my-release is the name of your release. Helm will generate all the Kubernetes resources and deploy them to your cluster.\nTo verify the installation:\nhelm list kubectl get all Step 8: Update and Manage Your Release #\rIf you need to make changes, update your chart files and run:\nhelm upgrade my-release ./my-first-chart To roll back to a previous version:\nhelm rollback my-release And when you\u0026rsquo;re done, clean up with:\nhelm uninstall my-release Conclusion #\rCongratulations! You\u0026rsquo;ve just created your first Helm chart. We covered the basic structure of a chart, how to define values and templates, and how to install and manage releases. Helm charts might seem complex at first, but once you understand the basics, they become an invaluable tool for managing Kubernetes applications across multiple environments.\nThe key takeaway is that Helm lets you write your Kubernetes manifests once and reuse them everywhere by simply changing values. This makes your deployments more consistent, maintainable, and less error-prone. As you get more comfortable, you can explore advanced features like chart dependencies, hooks, and custom functions to make your charts even more powerful.\n","date":"29 August 2025","externalUrl":null,"permalink":"/blogs/blog2-first-helm-chart/","section":"","summary":"","title":"How to Create Your First Helm Chart","type":"blogs"},{"content":" DevOps / Site Reliability Engineer with strong experience operating and supporting production-scale distributed systems across cloud and on-prem environments. Proven ability to build and operate reliable, observable, and secure platforms, with hands-on expertise in CI/CD pipelines, AWS cloud infrastructure, Infrastructure as Code (Terraform), and Kubernetes-based containerized workloads. Trusted partner to software engineering teams, driving improvements in system operability, deployment consistency, and developer experience. Passionate about automation, reliability engineering, and operational excellence. ","externalUrl":null,"permalink":"/about/","section":"","summary":"","title":"","type":"page"},{"content":"","externalUrl":null,"permalink":"/about/videos/","section":"","summary":"","title":"","type":"about"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]